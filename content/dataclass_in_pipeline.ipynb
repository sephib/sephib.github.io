{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidying up Pipelines with DataClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Tidy code makes everyones life easier.  \n",
    "The code we write will be read many times, so making things easier to mange will be apprciated later on by everyone on the team.  \n",
    "Some tools that can assist in this cleanliness is the usage of Pipelines and Dataclasses.  \n",
    "\n",
    "> MLEngineer is 10% ML 90% Engineer.   \n",
    "\n",
    "### Pipeline\n",
    "Pipeline is a *meta* object that assists in managing the processes in a ML model.  Pipelines can encapsulat seperate processes which can later on be combined together.       \n",
    "Forcing to work with [Pipline objects](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) can be nuisance at the begining (especially the conversion between  `pandas DataFrame` and `np.ndarray` ), but it garenties the quility of the model down the line (no data leakage, modularity etc.). Here is [Kevin Markham 4 min. video](https://www.youtube.com/watch?v=yv4adDGcFE8) explaining the pipeline advantages.   \n",
    "\n",
    "### Dataclass\n",
    "Another usefull ML coding tool is to use an object to save datasets along the pipeline. Before Python 3.7 you may have been using [namedtubple](https://docs.python.org/3.9/library/collections.html?highlight=namedtuple#collections.namedtuple), however since Python 3.7 [dataclasses](https://docs.python.org/3/library/dataclasses.html) was introduced, which are a great candidate for storing such data objects. Using dataclasses allows for consistancy while accessing various datasets along the Pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "In this post we will be showing an advance pipeline that incorperates preprocess per column type,  handle the Categorical columns using the [vtreat](https://github.com/WinVector/pyvtreat) package, and then run a [catboost](https://catboost.ai/) classifier.  \n",
    "\n",
    "\n",
    "Lets assume that we have a classification problem and our data has numeric and categorical column types, so we may build our pipeline as follows:  \n",
    "\n",
    "\n",
    "###  Pipeline\n",
    "\n",
    "```python\n",
    "y = df.pop(\"label\")\n",
    "X = df.copy(True)\n",
    "\n",
    "num_pipe = Pipeline([(\"scaler\",StanderdScaler()),\n",
    "                     (\"variance\",VarianceThreshold()),\n",
    "                     ])\n",
    "preprocess_pipe = ColumnTransformer(\n",
    "   remainder=\"passthrough\",\n",
    "   transformers=[(\"num_pipe\", num_pipe, X.select_dtypes(\"number\"))]\n",
    ")                     \n",
    "\n",
    "pipe = Pipeline([(\"preprocess_pipe\", preprocess_pipe),\n",
    "               (\"vtreat\", BinomiaOutcomeTreatmentPlan()),\n",
    "])                 \n",
    "\n",
    "```  \n",
    "\n",
    "In this *psedo code* our Pipeline has some preprocessing to the numeric columns follwing by the processing of the categrical columns with the vtreat package (it will passthrough all the non categorical and numeric columns).\n",
    "\n",
    "Since `catboost` does not have a transform method we are going to introduce it leater on.  \n",
    "Additionaly since we have an imbalanced data set we are going to use the `StratifiedShuffleSplit` when splitting our data.  \n",
    "\n",
    "So now the time has come to [cut up our data](https://getyarn.io/yarn-clip/2c689f11-6d71-425c-a701-81be09ad034e#llil9DAFRQ.copy)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test vs. Train vs. Valid\n",
    "A common workflow when developing an ML model is the necessity to split the date into [Test/Train/Valid datasets](https://machinelearningmastery.com/difference-test-validation-datasets/).   \n",
    "In a nut shell the difference between the data are:  \n",
    "1. Test - put aside - don't look until final model estimation  \n",
    "2. Train - dataset to train model   \n",
    "3. Valid - dataset to validate model during the training phase (this can be via iteration, GridSearch or preventing overfitting )   \n",
    "\n",
    "Each dataset will have similar attributes that we will need to save and access throughout the ML workflow.  \n",
    "In order to prevent confusion lets create a `dataclass` to save the datasets in a structured manner.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic dataclass \n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Split:\n",
    "    X: np.ndarray = None\n",
    "    y: np.array = None\n",
    "    idx: np.array = None\n",
    "    pred_class: np.array = None\n",
    "    pred_proba: np.ndarray = None\n",
    "    kwargs: Dict = None\n",
    "    \n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the training and test datasets as follows:  \n",
    "```python\n",
    "train = Split(name='train')   \n",
    "test = Split(name='test')  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `dataclass` will have the follwing attributes:  \n",
    "1. `X` - a numpy ndarray storing all the features   \n",
    "2. `y` - a numpy array storing the labeling classification  \n",
    "3. `idx` - the index for storing the original indexes (usefull for referencing at the end of the pipe line )  \n",
    "4. `pred_class` - a numpy array storing the predicted classification  \n",
    "5. `pred_proba` - a numpy ndarray for storrying the probabilites of the classificaitons\n",
    "\n",
    "Additionally we will store a `name` for the dataclass to easily referencing it along the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting In Action  \n",
    "There are several methods that you can split your datasets. When data is imbalenced it is important to split the data with a stratified method,  so in our case we chose to use [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html).  In contrast  to the simple [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train%20split#sklearn.model_selection.train_test_split) which returnes the datasets themselfs, the StratifiedShuffleSplit returnes only the indices for each group. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for fold_name, (train.idx, test.idx) in enumerate( StratifiedSplitValid(X, y, n_split=5, train_size=0.8) ):\n",
    "    train, test = get_split_from_idx(X, y, train, test)  # a helper function to get all data itslef since the StratifiedSplitValid on returns the indices\n",
    "    _train_X = pipe.fit_transform(train.X)\n",
    "```  \n",
    "Our helper function is nice and minimal thx for the usage of our `dataclasses`  \n",
    "\n",
    "```python\n",
    "def get_split_from_idx(X, y, split1: Split, split2: Split):\n",
    "    split1.X, split2.X = X.iloc[split1.idx], X.iloc[split2.idx]\n",
    "    split1.y, split2.y = y.iloc[split1.idx], y.iloc[split2.idx]\n",
    "    return split1, split2  \n",
    "```    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline in action\n",
    "Now we can run the first part of our Pipeline  \n",
    "```python\n",
    "_train_X = pipe.fit_transform(train.X)  \n",
    "\n",
    "```\n",
    "\n",
    "Once we have fit_transformed our data (allowing for vtreat magic to work), we can introduce `catboost` into our Pipeline.  \n",
    "\n",
    "```python\n",
    "catboost_clf = CatBoostClassifier()\n",
    "\n",
    "train_vlaid = Split(name=\"train_vlaid\")\n",
    "vlaid = Split(name=\"vlaid\")\n",
    "for fold_name, (train_vlaid.idx, vlaid.idx) in enumerate(StratifiedSplitValid(_train_X, train.y, n_split=5, train_size=0.9) ):\n",
    "    train_vlaid, vlaid = get_split_from_idx(_train_X, train.y, train_vlaid, vlaid)\n",
    "\n",
    "    pipe.steps.append((\"catboost_clf\",catboost_clf))\n",
    "    \n",
    "    pipe.fit(train_size.X, train_vlaid.y,\n",
    "            catboost_clf__eval_set=[(valid.X, valid.y)],\n",
    "    )\n",
    "    \n",
    "```\n",
    "Notice the two following things:  \n",
    "1. Using `pipe.steps.append` we are able to introduce steps into the pipeline that could not be initially part of the workflow.  \n",
    "2. Adding paramters into the steps within the pipeline requies the usage of double dunder for [nested paramters](https://scikit-learn.org/stable/modules/compose.html#nested-parameters).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can get some result  \n",
    "\n",
    "```python\n",
    "test.pred_class = pipe.(test.X)  \n",
    "test.pred_proba = pipe.pred_proba(test.X)[:,1]\n",
    "```  \n",
    "\n",
    "Now lets say we want to analyse our model and analyse some specific observations or generate our confusion_matrix we can run the following code:  \n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix_test = confusion_matrix(y_true=test.y, y_pred=test.pred_class )\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "\n",
    "This blog post outlined the advantages for using Pipelines and Dataclasses.  \n",
    "\n",
    "I hope the example illustrated the potentail for such usage and inspires you to try them out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
